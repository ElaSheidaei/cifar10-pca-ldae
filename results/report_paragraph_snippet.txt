Then, for additional experiments, I implemented a simplified l-DAE-style prototype on CIFAR-10 to test whether the paper’s latent-dimension trends appear in a small-scale setting. I used patch-wise PCA as a tokenizer (4×4 patches), added timestep-dependent Gaussian noise in latent space, trained a small denoising autoencoder, and evaluated the learned representations with a linear probe on the encoder features. 

In a fast sweep using 20k training samples and 10 AE epochs, linear-probe accuracy was 37.84% (d=8), 37.48% (d=16), and 35.38% (d=32). Under this limited training budget, small-to-moderate latent dimensionality performed similarly, while the larger latent space degraded, consistent with the idea that an appropriately compressed latent space can be beneficial for this objective. 

For qualitative inspection, I additionally trained the d=16 model on the full CIFAR-10 training set for 20 epochs and visualized (i) clean vs pixel-noised vs latent-noised inputs and (ii) clean → latent-noised → denoised reconstructions, highlighting that latent-space corruption is structured (patch-dependent) and that the denoiser recovers global structure while smoothing some fine details. Implementation details and figures are provided in the attached notebooks and repository.
